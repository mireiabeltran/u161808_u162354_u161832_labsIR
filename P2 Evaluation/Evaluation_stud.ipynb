{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval and Web Analytics: Evaluation\n",
    "\n",
    "## Rank-Based Metrics\n",
    "\n",
    "\n",
    "Welcome to the second practical session! In this session you are going to implement some common metrics used for the evaluation of an information retrieval system.\n",
    "\n",
    "\n",
    "### Unranked retrieval evaluation: Precision and Recall\n",
    "\n",
    "**Precision:** Precision is the fraction of the retrieved documents (the set A) which is relevant.\n",
    "\n",
    "$$\\begin{equation}\n",
    "  Precision=\\frac{{|R \\cap A|}}{{|A|}}\n",
    "\\end{equation} \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**Recall:** is the fraction of the relevant documents (the set R) which has been retrieved.\n",
    "\n",
    "$$\\begin{equation}\n",
    "  Recall=\\frac{{|R \\cap A|}}{{|R|}}\n",
    "\\end{equation} \n",
    "$$\n",
    "\n",
    "<img src=\"images/prec_rec.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center> <u> <font color=''> Figure 2 </u><font color=''> : Precision and Recall</caption> \n",
    "\n",
    "The above definitions of precision and recall assumes that all docs in the set A have been examined. \n",
    "However, the user is not usually presented with all docs in the answer set A at once. User sees a ranked set of documents and examines them starting from the top. Thus, precision and recall vary as the user proceeds with their examination of the set A.\n",
    "\n",
    "\n",
    "If we want to benchmark different systems we need:\n",
    "\n",
    "- A collection of documents that have to be representative of the collection we expect to be in reality;\n",
    "- A collection of information needs (have to be representative of the collection we expect to be in reality);\n",
    "- Human relevance assessment for (information need, document) pairs.\n",
    "\n",
    "\n",
    "Information retrieval systems (like search engine) can express the relevance for a (information need, document pair) in a binary way or through multiple levels.\n",
    "\n",
    "Here follow some common metrics used for the evaluation of a retrieval system:\n",
    "\n",
    "### Rank-Based Measures\n",
    "- ##### Binary relevance\n",
    "    - Precision@K (P@K)\n",
    "    - Average Precision@K (P@K)\n",
    "    - Mean Average Precision (MAP) \n",
    "    - Mean Reciprocal Rank (MRR)\n",
    "    \n",
    "- ##### Multiple levels of relevance\n",
    "    - Normalized Discounted Cumulative Gain (NDCG)\n",
    "\n",
    "Notice that we only mentioned precision but not recall. Indeed, by returning all the documents for a query will result in a trivial 100% recall, Thus recall by itself is commonly not used as a metric in this context.\n",
    "    \n",
    "We are going to test the above metrics on a ranking of results which is stored in the ```inputs/test_predictions.csv``` file. The prediction dataset contains:\n",
    "\n",
    "- q_id: query id.\n",
    "- doc_id: document id.\n",
    "- predicted_relevance: relevance predicted through a ranking algorithm.\n",
    "- y_true: actual score of the document for the query (ground truth)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Load data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>predicted_relevance</th>\n",
       "      <th>y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.637926</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.824241</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.358856</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.096755</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.268338</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   q_id  doc_id  predicted_relevance  y_true\n",
       "0     0       0            -0.637926     2.0\n",
       "1     0       1            -0.824241     1.0\n",
       "2     0       2            -1.358856     3.0\n",
       "3     0       3            -0.096755     1.0\n",
       "4     0       4            -1.268338     0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results = pd.read_csv(\"inputs/test_predictions.csv\")\n",
    "search_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that out ground truth consists of multiple levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ground truth of our dataset is composed of 5 Relevance Levels: [0.0, 1.0, 2.0, 3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"The ground truth of our dataset is composed of {} Relevance Levels: {}\" .format(len(search_results[\"y_true\"].unique()), sorted(search_results[\"y_true\"].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute *Precision@K, Mean Average Precision and Mean Reciprocal Rank*, we need binary relevance (relevant-1, not relevant-0).\n",
    "\n",
    "\n",
    "To simplify the task, we will consider as **relevant all documents that have actual score (y_true) equal or higher than $2$, and not-relevant the remaining documents**. \n",
    "\n",
    "Add a column ```bin_y_true``` following the above rule to ```search_results```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>predicted_relevance</th>\n",
       "      <th>y_true</th>\n",
       "      <th>bin_y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.637926</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.824241</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.358856</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.096755</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.268338</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   q_id  doc_id  predicted_relevance  y_true  bin_y_true\n",
       "0     0       0            -0.637926     2.0           0\n",
       "1     0       1            -0.824241     1.0           0\n",
       "2     0       2            -1.358856     3.0           1\n",
       "3     0       3            -0.096755     1.0           0\n",
       "4     0       4            -1.268338     0.0           0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results[\"bin_y_true\"] = search_results[\"y_true\"].apply(lambda y:1 if y>2 else 0)\n",
    "search_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Metrics\n",
    "\n",
    "\n",
    "#### Precision@K (P@K)\n",
    "\n",
    "Precision at k **(P@k) measures the number of relevant results among the top k documents**. It assess whether the users are getting relevant documents at the top of the ranking or not.\n",
    "\n",
    "A drawback of this metric is that it fails to take into account the positions of the relevant documents among the top k.\n",
    "\n",
    "Implement the function ```precision_at_k(y_true, y_score, k)``` that takes as input the true relevance labels, the predicted score, the number of docs to consider k and compute the precision as $k$.\n",
    "\n",
    "Steps:\n",
    "1. use ```np.argsort``` and [::1] to obtain the list of indexes of the predicted score sorted in descending order.\n",
    "2. use the indexes of point 1. to sort the actual relevance label of the documents (hint: ```np.take```).\n",
    "3. consider the top k relevance label of the documents (after the sorting) and retrieve the number of relevant documents (among the top k, i.e., normalise the number of relevant documents by k).\n",
    "\n",
    "Notice that the Precision@K is computed for a single query and the respective set of retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, y_score, k=10):\n",
    "    '''    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    precision @k : float\n",
    "    \n",
    "    '''    \n",
    "    print(y_score)\n",
    "    order = np.argsort(y_score)[::1]\n",
    "    y_true = np.take(y_true,order[:k])\n",
    "    relevant = sum(y_true==1)\n",
    "    return relevant/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-56b2b2a9380b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_score' is not defined"
     ]
    }
   ],
   "source": [
    "y_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute precisio@10 for query with q_id=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     -0.637926\n",
      "1     -0.824241\n",
      "2     -1.358856\n",
      "3     -0.096755\n",
      "4     -1.268338\n",
      "         ...   \n",
      "133    0.460846\n",
      "134   -0.415846\n",
      "135    0.374341\n",
      "136   -1.290106\n",
      "137    0.205322\n",
      "Name: predicted_relevance, Length: 138, dtype: float64\n",
      "==> Precision@5: 0.2\n",
      "\n",
      "\n",
      "Check on the dataset sorted by score:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>predicted_relevance</th>\n",
       "      <th>y_true</th>\n",
       "      <th>bin_y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>1.705258</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>1.116369</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>1.096797</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1.084367</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>1.082985</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     q_id  doc_id  predicted_relevance  y_true  bin_y_true\n",
       "88      0      88             1.705258     2.0           0\n",
       "114     0     114             1.116369     2.0           0\n",
       "63      0      63             1.096797     1.0           0\n",
       "34      0      34             1.084367     1.0           0\n",
       "86      0      86             1.082985     3.0           1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for query 0\n",
    "\n",
    "current_query = 0\n",
    "current_query_res = search_results[search_results[\"q_id\"] == current_query] \n",
    "k=5\n",
    "\n",
    "print(\"==> Precision@{}: {}\\n\".format(k,\n",
    "                                precision_at_k(current_query_res[\"bin_y_true\"], current_query_res[\"predicted_relevance\"], k)))\n",
    "\n",
    "\n",
    "print(\"\\nCheck on the dataset sorted by score:\\n\")\n",
    "current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Precision@3: 0.0\n",
      "\n",
      "==> Precision@10: 0.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k=3\n",
    "print(\"==> Precision@{}: {}\\n\".format(k,\n",
    "                                precision_at_k(current_query_res[\"bin_y_true\"], current_query_res[\"predicted_relevance\"], k)))\n",
    "\n",
    "\n",
    "k=10\n",
    "print(\"==> Precision@{}: {}\\n\".format(k,\n",
    "                                precision_at_k(current_query_res[\"bin_y_true\"], current_query_res[\"predicted_relevance\"], k)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Precision@k - AP@K\n",
    "\n",
    "\n",
    "With respect to $P@K$, $AP@K$ gives a better intuition of the model ability of sorting the results for a specific query. It tells how much the relevant documents are concentrated in the highest ranked predictions.\n",
    "\n",
    "The Average Precision approximates the area under the uninterpolated precision-recall curve.\n",
    "\n",
    "$$AP@K=\\frac{1}{GTP}\\sum_k^n{P@K \\times rel@K}\\tag{1}$$\n",
    "\n",
    "where: \n",
    "- GTP is the total number of ground truth positives;\n",
    "- P@k is the precision@k \n",
    "- rel@k is a relevance function; it retrievs 1 if the document at rank k is relevant and 0 otherwise.\n",
    "\n",
    "<img src=\"images/apk.png\" style=\"width:600px;height:250px;\">\n",
    "<caption><center> <u> <font color=''> Figure 1 </u><font color=''>  : Computation of AP <br> (Picture taken from <i>https://towardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52</i>)</center></caption>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function ```avg_precision_at_k(y_true, y_score, k)``` that takes as input the true relevance labels, the predicted score, the number of docs to consider k and compute the average precision at ùëò.\n",
    "\n",
    "Notice that the Precision@K is computed for a single query and the respective set of retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_precision_at_k(y_true, y_score, k=10):\n",
    "    \n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    average precision @k : float\n",
    "    '''\n",
    "    \n",
    "    gtp = np.sum(y_true == 1)\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])         \n",
    "\n",
    "    ## if all docs are not relevant\n",
    "    if gtp==0:\n",
    "        return 0\n",
    "    n_relevant_at_i = 0\n",
    "    prec_at_i = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == 1:\n",
    "            n_relevant_at_i += 1\n",
    "            prec_at_i += n_relevant_at_i / (i+1)\n",
    "    return prec_at_i/gtp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute precisio@10 for query with q_id=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15432511227002754"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_precision_at_k(np.array(current_query_res[\"bin_y_true\"]), np.array(current_query_res[\"predicted_relevance\"]), 150)\n",
    "#avg_precision_at_k(np.array([1,0,0,1,1,0]), np.array([0.9,0.8,0.7,0.6,0.5, 0.4]),6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15432511227002754"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check with average_precision_score of sklearn\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "k = 150\n",
    "temp = current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)\n",
    "average_precision_score(np.array(temp[\"bin_y_true\"]), np.array(temp[\"predicted_relevance\"][:k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check with average_precision_score of sklearn\n",
    "\n",
    "y_true = np.array([1, 1, 0, 1, 0, 0, 1])\n",
    "y_scores = np.array([7, 6, 5, 4, 3, 2, 1])\n",
    "assert(average_precision_score(y_true, y_scores) == avg_precision_at_k(y_true, y_scores,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8303571428571428"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_precision_at_k(y_true, y_scores,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manual check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05625"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_precision_at_k(np.array(current_query_res[\"bin_y_true\"]), np.array(current_query_res[\"predicted_relevance\"]), 10) #he canviat , hi havia score al final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>predicted_relevance</th>\n",
       "      <th>y_true</th>\n",
       "      <th>bin_y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>1.705258</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>1.116369</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>1.096797</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1.084367</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>1.082985</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>1.081464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>1.075457</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>1.063326</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1.016901</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>0.906784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     q_id  doc_id  predicted_relevance  y_true  bin_y_true\n",
       "88      0      88             1.705258     2.0           0\n",
       "114     0     114             1.116369     2.0           0\n",
       "63      0      63             1.096797     1.0           0\n",
       "34      0      34             1.084367     1.0           0\n",
       "86      0      86             1.082985     3.0           1\n",
       "47      0      47             1.081464     0.0           0\n",
       "55      0      55             1.075457     2.0           0\n",
       "76      0      76             1.063326     3.0           1\n",
       "17      0      17             1.016901     2.0           0\n",
       "58      0      58             0.906784     1.0           0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5578869047619048"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "(1+(2/2)+(3/5)+(4/7)+(5/8)+(6/9))/np.sum(current_query_res[\"bin_y_true\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Average Precision (MAP)\n",
    "\n",
    "The **mAP** is simply the mean of all the queries AP. This metric is not computed for a single query as previous metrics but it takes into account all queries.\n",
    "\n",
    "Above we mentioned that the average precision approximates the area under the uninterpolated precision-recall curve for a single query. As a consequence, the MAP is roughly the average area under the precision-recall curve for a set of queries.\n",
    "\n",
    "$$mAP=\\frac{1}{N}\\sum_{i=1}^n{AP_i}\\tag{2}$$\n",
    "\n",
    "<img src=\"images/map.png\" style=\"width:600px;height:450px;\">\n",
    "<caption><center> <u> <font color=''> Figure 2 </u><font color=''>  : Computation of mAP </center></caption>  \n",
    "\n",
    "Implement a function ```map_at_k(search_res, k)``` that takes as input the dataset containing search results (list of actual labels, list of predicted scores, list queries) and k, and compute the Mean Average Precision (MAP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_at_k(search_res, k=10):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    search_res: search results dataset containing:\n",
    "        q_id: query id.\n",
    "        doc_id: document id.\n",
    "        predicted_relevance: relevance predicted through LightGBM.\n",
    "        y_true: actual score of the document for the query (ground truth).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mean average precision @k : float\n",
    "    '''\n",
    "    avp = []\n",
    "    for q in search_res[\"q_id\"].unique(): #loop over all query id\n",
    "        curr_data = search_res[search_res[\"q_id\"]==q ]  # select data for current query\n",
    "        avp.append(avg_precision_at_k(np.array(curr_data[\"bin_y_true\"]), np.array(curr_data[\"predicted_relevance\"]),k)) #append average precision for current query\n",
    "    return np.sum(avp)/len(avp) # return mean average precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute mAP@10 for all queries of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15492345071483846"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_k = map_at_k(search_results,10)\n",
    "map_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Reciprocal Rank (MRR)\n",
    "\n",
    "Mean Reciprocal Rank is particularly used when we are interested in the first correct answer.\n",
    "\n",
    "If we define:\n",
    "\n",
    "- $R_i$ as the ranking for the query $q_i$;\n",
    "- $S_correct(R_i)$ as the position of the first correct answer in $R_i$\n",
    "- $K$ as the threshold for ranking position\n",
    "\n",
    "The reciprocal rank $$RR(R_i)$$ for query $q_i$ is computed as follows:\n",
    "\n",
    "$$\\begin{equation}\n",
    "  RR(R_i)==\\left\\{\n",
    "  \\begin{array}{@{}ll@{}}\n",
    "    \\frac{1}{S_{correct}(R_i)}, & \\text{if}\\ S_{correct}(R_i) $\\leq$ K \\\\\n",
    "    0, & \\text{otherwise}\n",
    "  \\end{array}\\right.\n",
    "  \\tag{3}\n",
    "\\end{equation} \n",
    "$$\n",
    "\n",
    "\n",
    "The Mean Reciprocal Rank (MRR) can be defined as the mean of the RR for all queries:\n",
    "\n",
    "$$\\begin{equation}\n",
    "  MRR(R_i)==\\frac{1}{N}\\sum_{i=1}^N{RR(R_i)}\n",
    "\\end{equation} \n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "where $N$ is the total number of queries (and rankings since we have a ranking per query).\n",
    "\n",
    "Implement the function ```rr_at_k(y_true, y_score, k)``` that computes the Reciprocal Rank at the threshold $k$ for a single query and then compute the MRR@K for k=3, 5 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rr_at_k(y_true, y_score, k=10):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Reciprocal Rank for qurrent query\n",
    "    '''\n",
    "\n",
    "    order = np.argsort(y_score)[::-1] # get the list of indexes of the predicted score sorted in descending order.\n",
    "    y_true =np.take(y_true,order[:k]) # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k.\n",
    "    if np.sum(y_true)== 0: # if there are not relevant doument return 0\n",
    "        return 0\n",
    "    return 1/(np.argmax(y_true == 1) +1 ) # hint: to get the position of the first relevant document use \"np.argmax\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.array([0,1,0,1,1])\n",
    "score = np.array([0.9, 0.5, 0.6, 0.7, 0.2])\n",
    "rr_at_k(y_true, score,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make some test with the query with q_id = 8 to check if your function is working properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>predicted_relevance</th>\n",
       "      <th>y_true</th>\n",
       "      <th>bin_y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>8</td>\n",
       "      <td>52</td>\n",
       "      <td>0.115248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>-0.046405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.404693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>-0.493206</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.701708</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.755329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.802263</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.827835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.836900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.878972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      q_id  doc_id  predicted_relevance  y_true  bin_y_true\n",
       "1067     8      52             0.115248     0.0           0\n",
       "1039     8      24            -0.046405     0.0           0\n",
       "1028     8      13            -0.404693     0.0           0\n",
       "1051     8      36            -0.493206     1.0           0\n",
       "1066     8      51            -0.701708     1.0           0\n",
       "1034     8      19            -0.755329     0.0           0\n",
       "1015     8       0            -0.802263     2.0           0\n",
       "1025     8      10            -0.827835     0.0           0\n",
       "1031     8      16            -0.836900     0.0           0\n",
       "1022     8       7            -0.878972     0.0           0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_query = 8\n",
    "current_query_res = search_results[search_results[\"q_id\"] == current_query] \n",
    "current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array(search_results[search_results['q_id'] == 8][\"bin_y_true\"])\n",
    "scores = np.array(search_results[search_results['q_id'] == 8][\"predicted_relevance\"])\n",
    "np.round(rr_at_k(labels, scores, 10),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute the MRR@K for k=3,5,and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr = {}\n",
    "for k in [3,5,10]:\n",
    "    RRs = []\n",
    "    for q in search_results['q_id'].unique() : # loop over all query ids\n",
    "        labels = np.array(search_results[search_results['q_id'] == q][\"bin_y_true\"]) # get labels for current query\n",
    "        scores =  np.array(search_results[search_results['q_id'] == q][\"predicted_relevance\"])# get predicted score for current query\n",
    "        RRs.append(rr_at_k(labels,scores,k)) # append RR for current query\n",
    "    mrr[k] = np.round(float(sum(RRs)/len(RRs)),4) # Mean RR at current k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 0.3012, 5: 0.3121, 10: 0.3213}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple levels of relevance metrics\n",
    "\n",
    "#### NDCG - Normalized Discounted Cumulative Gain\n",
    "\n",
    "**NDCG** also works if document relevances are a real number, i.e., when each document's relevance is note expressed in a binary form (relevant or non-relevant).\n",
    "\n",
    "This metric is especially used with machine learning based approaches, like Learning To Rank, and it takes values between $0$ (very poor/bad ranking) and $1$ (optimal ranking). \n",
    "\n",
    "Before defining $NDCG$, let's talk about **DCG**.\n",
    "**DCG** is based on the following assumptions:\n",
    "\n",
    "- Highly relevant documents are more useful when appearing earlier in a search engine result list (have higher ranks)\n",
    "- Highly relevant documents are more useful than marginally relevant documents, which are in turn more useful than non-relevant documents.\n",
    "\n",
    "$DCG$ is based on the notion of **CG (Cumulative Gain)**. \n",
    "\n",
    "**Cumulative Gain (CG):** does not include the position of a result in the consideration of the usefulness of a result set. It is the sum of the relevance values of all results in a search result list (in the ranking). Suppose you were presented with a set of search results for a query and asked to rank each result:\n",
    "\n",
    "- 0 => Not relevant \n",
    "- 1 => Near relevant \n",
    "- 2 => Relevant.\n",
    "\n",
    "If we sum the values for a page of results we will have a measure of the cumulative gain (CG).\n",
    "\n",
    "$$CG = \\sum_{pos=1}^n Rel_{pos}\\tag{5}$$\n",
    "\n",
    "Where $Rel_{pos}$ is the graded relevance of $pos^{th}$ document.\n",
    "Cumulative gain, however, does not reward relevant results that appear higher in the result set (CG function is unaffected by changes in the ordering of search results). To achieve the Discounted cumulative gain (DCG) we must discount results that appear lower.\n",
    "\n",
    "**Discounted Cumulative Gain (DCG):** The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result.\n",
    "\n",
    "$$DCG = \\sum_{pos=1}^n \\frac{Rel_{pos}}{\\log_2(pos+1)} = Rel_1 + \\sum_{pos=2}^n \\frac{Rel_{pos}}{\\log_2(pos+1)}\\tag{6}$$\n",
    "\n",
    "An alternative formulation of $DCG$ that places stronger emphasis on retrieving relevant documents is the following:\n",
    "\n",
    "$$DCG = \\sum_{pos=1}^n \\frac{2^{Rel_{pos}} -1}{\\log_2(pos+1)}\\tag{7}$$\n",
    "\n",
    "The latter formula is commonly used in industry including major web search companies. These two formulations of DCG are the same when the relevance values of documents are binary.\n",
    "\n",
    "**Normalized DCG (NDCG):** If you calculate DCG for different queries you‚Äôll find that some queries are just harder than others and will produce lower DCG scores than easier queries. Normalization solves this problem by scaling the results based off of the best result seen (Ideal DCG or $IDCG$). This is done by sorting all relevant documents in the corpus by their relative relevance, producing the maximum possible DCG through position $n$, also called Ideal DCG (IDCG) through that position (*usually it is the ground truth*). \n",
    "\n",
    "$$NDCG_{pos} = \\frac{DCG_{pos}}{iDCG}\\tag{8}$$\n",
    "\n",
    "<img src=\"images/ndcg.png\" style=\"width:650px;height:450px;\">\n",
    "<caption><center> <u> <font color=''> Figure 2 </u><font color=''> : Computation of NDCG </br> \n",
    "    (Picture taken from https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832)</center></caption>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the functions: \n",
    "- ```dcg_at_k(y_score, y_true, k)``` based on formula $7$  \n",
    "- ```ndcg_at_k(y_score, y_true, k)```\n",
    "\n",
    "Compute:\n",
    "- the $NDCG@10$ for query with ```q_id=0```\n",
    "- the average $NDCG@10$ (considering all queries/rankings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(y_true, y_score,  k=10):\n",
    "    order = np.argsort(y_score)[::-1] # get the list of indexes of the predicted score sorted in descending order.\n",
    "    y_true = np.take(y_true, order[:k]) # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k.\n",
    "    gain = 2**y_true -1 # Compute gain (use formula 7 above)\n",
    "    discounts = np.log2(np.arange(len(y_true))+2) # Compute denominator\n",
    "    return np.sum(gain / discounts) #return dcg@k\n",
    "\n",
    "\n",
    "def ndcg_at_k(y_true, y_score, k=10):    \n",
    "    dcg_max = dcg_at_k(y_true, y_true, k) # Ideal dcg\n",
    "    if not dcg_max:\n",
    "        return 0\n",
    "    return np.round(dcg_at_k(y_true, y_score, k)/dcg_max,4)  # return ndcg@k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the  ùëÅùê∑ùê∂ùê∫@10 for query with q_id=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg@10 for query with q_id=0: 0.4392\n"
     ]
    }
   ],
   "source": [
    "q_id = 0\n",
    "k = 10\n",
    "labels = np.array(search_results[search_results['q_id'] == q_id][\"y_true\"])\n",
    "scores = np.array(search_results[search_results['q_id'] == q_id][\"predicted_relevance\"])\n",
    "ndcg_k = np.round(ndcg_at_k(labels, scores, k),4)\n",
    "print(\"ndcg@{} for query with q_id={}: {}\".format(k,q_id,ndcg_k))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the average  ùëÅùê∑ùê∂ùê∫@10  (considering all queries/rankings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ndcg@10: 0.4646\n"
     ]
    }
   ],
   "source": [
    "ndcgs = []\n",
    "k=10\n",
    "for q in search_results['q_id'].unique(): # loop over all query ids\n",
    "    labels = np.array(search_results[search_results['q_id'] == q][\"y_true\"]) ## get labels for current query\n",
    "    scores = np.array(search_results[search_results['q_id'] == q][\"predicted_relevance\"]) # get predicted score for current query\n",
    "    ndcgs.append(ndcg_at_k(labels,scores,k)) # append NDCG for current query\n",
    "\n",
    "avg_ndcg = np.round(float(sum(ndcgs)/len(ndcgs)),4) # Compute average NDCG\n",
    "print(\"Average ndcg@{}: {}\".format(k,avg_ndcg))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
