{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRWA Final Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to build a search engine implementing different indexing and ranking algorithms. This will be done using a file containing a set of tweets from the World Health Organization (@WHO).\n",
    "\n",
    "It will be divided in four parts:\n",
    "\n",
    "    1) Text processing\n",
    "    2) Indexing and ranking\n",
    "    3) Evaluation \n",
    "    4) User Interface and Web analytics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Students Group 9:\n",
    "- Mireia Beltran (U161808)\n",
    "- Cisco Orteu (U162354)\n",
    "- Laura Casanovas (U161832)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages\n",
    "\n",
    "We first import all the packages needed for text processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mire2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# if you do not have 'nltk', the following command should work \"python -m pip install nltk\"\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import json\n",
    "import regex as re \n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data into memory\n",
    "\n",
    "The dataset is stored in a txt file ```dataset_tweets_WHO.txt```and it contains a set of tweets in json format. We create tweets_data by using json.loads() function, which from a JSON string it can be parsed and it returns the content of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('dataset_tweets_WHO.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data=[]\n",
    "for line in text:\n",
    "    tweet=json.loads(line)\n",
    "    tweets_data.append(tweet)\n",
    "text.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to read and process each tweet, we create a dictionary in which we will have just one row, and each column will contain one tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build DataFrame of tweet texts and languages\n",
    "df = pd.DataFrame(tweets_data)\n",
    "df.to_csv('df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a new variable called 'texts' which will contain in each position of the array a tweet. Below this cell we print as an example the content of the first position of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[]\n",
    "for i in df:\n",
    "    line =  df[i].item()['full_text']\n",
    "    texts.append(str(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's International Day for Disaster Risk Reduction\\n\\n#OpenWHO has launched a multi-tiered core curriculum to help equip you with the competencies needed to work within public health emergency response.\\n\\nStart learning today &amp; be #Ready4Response:\\n👉 https://t.co/hBFFOF0xKL https://t.co/fgZY22RWuS\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the function ```build_terms(text)```.\n",
    "\n",
    "It takes as input a text and performs the following operations:\n",
    "\n",
    "- Stem terms\n",
    "- Remove stop words\n",
    "- Remove punctuation \n",
    "- Remove links\n",
    "- Remove emojis\n",
    "- Transform all text to lowercase\n",
    "- Tokenize the text to get a list of terms\n",
    "\n",
    "(We decided not to remove hashtags since it may be interesting to treat them separately later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_terms(text):\n",
    "    \"\"\"\n",
    "    Preprocess the article text (title + body) removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    text -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    text - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "    # create the pattern\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\"#\", \"–\")# don't remove hashtags\n",
    "    remove = remove+'¿'\n",
    "    pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "    text = re.sub(pattern, \"\", text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    #compile a regular expression pattern into a regular expression object\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    #Return the string obtained by replacing the leftmost non-overlapping occurrences of pattern in string by the replacement\n",
    "    text = emoji_pattern.sub(r'', text) # no emoji\n",
    "    \n",
    "    # Transform in lowercase\n",
    "    text=  str.lower(text) \n",
    "    # Tokenize the text to get a list of terms\n",
    "    text=  text.split() \n",
    "    # Eliminate the stopwords\n",
    "    text=[l for l in text if l not in stop_words] \n",
    "    \n",
    "\n",
    "    # Perform stemming \n",
    "    text=[stemmer.stem(word) for word in text]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_processed = []\n",
    "for i in range(len(texts)):\n",
    "    texts_processed.append(build_terms(texts[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(texts, num_documents):\n",
    "    \"\"\"\n",
    "    Implement the inverted index\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of documents where these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    index = defaultdict(list)\n",
    "    tf = defaultdict(list)  #term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "    df = defaultdict(int)  #document frequencies of terms in the corpus\n",
    "    title_index = {}  # dictionary to map page titles to page ids\n",
    "    idf = defaultdict(float)\n",
    "    tweet_id = 0\n",
    "    for text in texts:  # Remember, lines contain all documents\n",
    "        terms = build_terms(text) #page_title + page_text\n",
    "        tweet_id += 1\n",
    "        title_index[tweet_id]=text  \n",
    "        \n",
    "        ## ===============================================================        \n",
    "        ## create the index for the current page and store it in current_page_index (current_page_index)\n",
    "        ## current_page_index ==> { ‘term1’: [current_doc, [list of positions]], ...,‘term_n’: [current_doc, [list of positions]]}\n",
    "\n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ##\"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_page_index ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,4]], ‘information’: [1, [2]]}\n",
    "\n",
    "        ## the term ‘web’ appears in document 1 in positions 0, \n",
    "        ## the term ‘retrieval’ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "        doc_freq = {i:terms.count(i) for i in terms}\n",
    "        current_page_index = {}\n",
    "        \n",
    "        for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "            \n",
    "            try:\n",
    "                # if the term is already in the index for the current page (current_page_index)\n",
    "                # append the position to the corresponding list\n",
    "                \n",
    "        ## START CODE\n",
    "                current_page_index[term][1].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_page_index[term] = [tweet_id, array('I',[position])] #'I' indicates unsigned int (int in Python)\n",
    "        \n",
    "        norm = 0\n",
    "        for term, posting in current_page_index.items():\n",
    "            # posting will contain the list of positions for current term in current document. \n",
    "            # posting ==> [current_doc, [list of positions]] \n",
    "            # you can use it to infer the frequency of current term.\n",
    "            norm += len(posting[1]) ** 2\n",
    "        norm = math.sqrt(norm)\n",
    "            \n",
    "\n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in current_page_index.items():\n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1])/norm,4)) ## SEE formula (1) above\n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term] +=1 # increment DF for current term\n",
    "\n",
    "        #merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "\n",
    "        # Compute IDF following the formula (3) above. HINT: use np.log\n",
    "        for term in df:\n",
    "            idf[term] = np.round(np.log(float(num_documents/df[term])), 4)\n",
    "\n",
    "    return index, tf, df, idf, title_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 281.47 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "index, tf, df, idf, title_index = create_index(texts, len(texts))\n",
    "print(\"Total time to create the index: {} seconds\".format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index results for the term 'researcher': []\n",
      "\n",
      "First 10 Index results for the term 'research': \n",
      "[[23, array('I', [16])], [154, array('I', [22])], [172, array('I', [17])], [204, array('I', [5])], [211, array('I', [8])], [212, array('I', [5])], [222, array('I', [6])], [423, array('I', [12])], [429, array('I', [4])], [460, array('I', [23])]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Index results for the term 'researcher': {}\\n\".format(index['researcher']))\n",
    "print(\"First 10 Index results for the term 'research': \\n{}\".format(index['research'][:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(terms, docs, index, idf, tf, title_index):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    title_index -- mapping between page id and page title\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "\n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaining elements would became 0 when multiplied to the query_vector\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "    #HINT: use when computing tf for query_vector\n",
    "\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        ## Compute tf*idf(normalize TF as done with documents)\n",
    "        query_vector[termIndex]=query_terms_count[term]/query_norm * idf[term] \n",
    "\n",
    "        # Generate doc_vectors for matching docs\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [doc_index, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "\n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
    "            if doc in docs:\n",
    "                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term]  # TODO: check if multiply for idf\n",
    "\n",
    "    # Calculate the score of each doc \n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "    # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine similarity\n",
    "    # see np.dot\n",
    "    \n",
    "    doc_scores=[[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items() ]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    result_docs = [x[1] for x in doc_scores]\n",
    "    #print document titles instead if document id's\n",
    "    #result_docs=[ title_index[x] for x in result_docs ]\n",
    "    if len(result_docs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)\n",
    "    #print ('\\n'.join(result_docs), '\\n')\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, index):\n",
    "    \"\"\"\n",
    "    The output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query = build_terms(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "    ## START DODE\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\"                        \n",
    "            term_docs=[posting[0] for posting in index[term]]\n",
    "            # docs = docs Union term_docs\n",
    "            docs = docs.union(term_docs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    ranked_docs = rank_documents(query, docs, index, idf, tf, title_index)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'drtedro': 951,\n",
       " '#covid19': 723,\n",
       " 'amp': 635,\n",
       " 'health': 596,\n",
       " 'rt': 549,\n",
       " 'vaccin': 432,\n",
       " 'countri': 330,\n",
       " 'peopl': 300,\n",
       " 'support': 243,\n",
       " 'pandem': 233,\n",
       " 'global': 228,\n",
       " 'need': 227,\n",
       " 'live': 201,\n",
       " '#vaccinequ': 197,\n",
       " 'care': 151,\n",
       " 'help': 149,\n",
       " 'access': 138,\n",
       " 'world': 134,\n",
       " 'year': 133,\n",
       " 'work': 125,\n",
       " 'use': 125,\n",
       " 'new': 116,\n",
       " 'diseas': 115,\n",
       " 'emerg': 113,\n",
       " 'today': 112,\n",
       " 'death': 112,\n",
       " 'risk': 110,\n",
       " 'provid': 109,\n",
       " 'servic': 108,\n",
       " 'includ': 108,\n",
       " 'call': 105,\n",
       " 'million': 105,\n",
       " 'continu': 105,\n",
       " 'prevent': 104,\n",
       " 'actacceler': 104,\n",
       " 'protect': 102,\n",
       " 'one': 96,\n",
       " 'safe': 96,\n",
       " 'also': 95,\n",
       " 'everi': 95,\n",
       " 'must': 94,\n",
       " 'end': 93,\n",
       " 'respons': 92,\n",
       " 'make': 92,\n",
       " 'suppli': 91,\n",
       " 'around': 88,\n",
       " 'share': 87,\n",
       " 'develop': 83,\n",
       " 'thank': 81,\n",
       " 'time': 81,\n",
       " 'medic': 81,\n",
       " 'dr': 81,\n",
       " 'commit': 80,\n",
       " 'public': 79,\n",
       " 'get': 79,\n",
       " 'mani': 78,\n",
       " 'system': 77,\n",
       " 'un': 76,\n",
       " 'variant': 76,\n",
       " 'month': 75,\n",
       " 'know': 75,\n",
       " 'case': 75,\n",
       " 'dose': 74,\n",
       " 'join': 74,\n",
       " 'increas': 74,\n",
       " 'take': 73,\n",
       " 'commun': 72,\n",
       " 'ensur': 71,\n",
       " 'intern': 70,\n",
       " 'action': 69,\n",
       " 'first': 69,\n",
       " 'patient': 68,\n",
       " 'treatment': 68,\n",
       " 'like': 67,\n",
       " 'us': 67,\n",
       " 'it’': 67,\n",
       " 'whoafro': 66,\n",
       " 'olymp': 66,\n",
       " 'whoemro': 65,\n",
       " 'caus': 65,\n",
       " '#mentalhealth': 64,\n",
       " 'partner': 64,\n",
       " 'children': 64,\n",
       " 'high': 64,\n",
       " 'sever': 64,\n",
       " 'effect': 64,\n",
       " 'report': 63,\n",
       " 'women': 63,\n",
       " 'worker': 63,\n",
       " 'minist': 63,\n",
       " 'learn': 62,\n",
       " 'togeth': 62,\n",
       " 'qualiti': 61,\n",
       " 'popul': 60,\n",
       " 'region': 60,\n",
       " 'improv': 60,\n",
       " 'reduc': 58,\n",
       " 'essenti': 58,\n",
       " '#worldmentalhealthday': 57,\n",
       " 'tool': 57,\n",
       " 'target': 57,\n",
       " 'day': 56,\n",
       " '#afghanistan': 56,\n",
       " 'healthi': 56,\n",
       " 'who’': 56,\n",
       " 'nation': 56,\n",
       " 'technolog': 56,\n",
       " 'recommend': 56,\n",
       " 'reach': 55,\n",
       " '10': 55,\n",
       " 'right': 55,\n",
       " 'life': 55,\n",
       " 'last': 55,\n",
       " 'viru': 55,\n",
       " 'g20org': 55,\n",
       " 'deliv': 53,\n",
       " 'import': 53,\n",
       " 'hospit': 53,\n",
       " 'social': 52,\n",
       " 'test': 52,\n",
       " 'condit': 51,\n",
       " 'impact': 51,\n",
       " 'meet': 51,\n",
       " 'govern': 51,\n",
       " 'respond': 50,\n",
       " 'even': 50,\n",
       " 'keep': 50,\n",
       " 'save': 50,\n",
       " 'food': 50,\n",
       " 'challeng': 49,\n",
       " '#askwho': 49,\n",
       " 'way': 49,\n",
       " 'state': 49,\n",
       " '#agoal4al': 49,\n",
       " '#tokyo2020': 49,\n",
       " 'put': 48,\n",
       " 'strengthen': 48,\n",
       " 'week': 48,\n",
       " 'brief': 48,\n",
       " 'least': 47,\n",
       " '2021': 47,\n",
       " '#whoacademi': 47,\n",
       " 'ask': 46,\n",
       " 'sexual': 45,\n",
       " 'infect': 45,\n",
       " '1': 45,\n",
       " 'inform': 45,\n",
       " 'manag': 45,\n",
       " 'delta': 45,\n",
       " 'better': 44,\n",
       " 'equit': 44,\n",
       " 'plan': 44,\n",
       " 'level': 44,\n",
       " 'build': 44,\n",
       " 'member': 44,\n",
       " '#rc71afro': 44,\n",
       " 'achiev': 43,\n",
       " 'critic': 43,\n",
       " 'lead': 43,\n",
       " 'may': 43,\n",
       " 'futur': 42,\n",
       " 'product': 42,\n",
       " 'tokyo2020': 42,\n",
       " 'leader': 41,\n",
       " 'address': 41,\n",
       " 'receiv': 41,\n",
       " 'outbreak': 41,\n",
       " 'data': 41,\n",
       " 'manufactur': 41,\n",
       " '#covax': 41,\n",
       " 'almost': 41,\n",
       " 'leadership': 40,\n",
       " 'media': 40,\n",
       " 'would': 40,\n",
       " '#healthwork': 39,\n",
       " 'sinc': 39,\n",
       " 'ill': 39,\n",
       " 'hub': 39,\n",
       " 'set': 39,\n",
       " 'read': 39,\n",
       " 'let': 39,\n",
       " 'much': 39,\n",
       " 'find': 39,\n",
       " 'billion': 39,\n",
       " 'everyon': 39,\n",
       " 'coverag': 39,\n",
       " 'next': 39,\n",
       " 'group': 38,\n",
       " 'part': 38,\n",
       " 'expert': 38,\n",
       " 'prepared': 38,\n",
       " 'control': 38,\n",
       " 'discuss': 38,\n",
       " 'affect': 37,\n",
       " 'especi': 37,\n",
       " 'major': 37,\n",
       " 'facil': 37,\n",
       " '1⃣': 37,\n",
       " 'chang': 37,\n",
       " 'mental': 37,\n",
       " 'pahowho': 37,\n",
       " 'famili': 37,\n",
       " 'measur': 37,\n",
       " 'team': 37,\n",
       " 'inequ': 37,\n",
       " 'face': 37,\n",
       " '#whoimpact': 37,\n",
       " 'epidem': 37,\n",
       " 'start': 36,\n",
       " 'human': 36,\n",
       " 'incl': 36,\n",
       " 'violenc': 36,\n",
       " 'progress': 36,\n",
       " 'still': 36,\n",
       " 'remain': 36,\n",
       " 'fund': 36,\n",
       " '70': 36,\n",
       " 'africa': 36,\n",
       " 'urgent': 36,\n",
       " 'question': 35,\n",
       " 'explain': 35,\n",
       " '#healthforal': 35,\n",
       " 'follow': 35,\n",
       " '2': 35,\n",
       " 'avail': 35,\n",
       " 'person': 35,\n",
       " 'paralymp': 35,\n",
       " '#worldbreastfeedingweek': 35,\n",
       " 'die': 34,\n",
       " 'sustain': 34,\n",
       " 'promot': 34,\n",
       " 'far': 34,\n",
       " 'septemb': 34,\n",
       " '#breastfeed': 34,\n",
       " '#selfcar': 34,\n",
       " 'effort': 33,\n",
       " '3⃣': 33,\n",
       " 'someon': 33,\n",
       " 'less': 33,\n",
       " 'practic': 33,\n",
       " 'unicef': 33,\n",
       " 'other': 33,\n",
       " 'area': 33,\n",
       " 'invest': 32,\n",
       " 'come': 32,\n",
       " 'qampa': 32,\n",
       " '#airpollut': 32,\n",
       " 'creat': 32,\n",
       " 'well': 32,\n",
       " 'disabl': 32,\n",
       " 'crisi': 32,\n",
       " 'bring': 32,\n",
       " 'fight': 32,\n",
       " 'capac': 32,\n",
       " 'societi': 32,\n",
       " 'current': 32,\n",
       " 'opportun': 32,\n",
       " 'good': 31,\n",
       " 'number': 31,\n",
       " 'financ': 31,\n",
       " 'open': 31,\n",
       " 'mvankerkhov': 31,\n",
       " 'full': 31,\n",
       " 'econom': 31,\n",
       " 'concern': 31,\n",
       " 'launch': 30,\n",
       " 'vulner': 30,\n",
       " 'covid19': 30,\n",
       " 'organ': 30,\n",
       " '#climatechang': 30,\n",
       " 'research': 30,\n",
       " 'made': 30,\n",
       " 'wellb': 30,\n",
       " 'close': 30,\n",
       " 'scale': 30,\n",
       " 'key': 30,\n",
       " 'info': 30,\n",
       " 'yet': 30,\n",
       " 'transmiss': 30,\n",
       " 'age': 29,\n",
       " 'resourc': 29,\n",
       " '2⃣': 29,\n",
       " 'seek': 29,\n",
       " 'possibl': 29,\n",
       " '#malaria': 29,\n",
       " 'deliveri': 29,\n",
       " 'see': 29,\n",
       " 'emmanuelmacron': 29,\n",
       " 'clinic': 28,\n",
       " 'talk': 28,\n",
       " 'physic': 28,\n",
       " 'two': 28,\n",
       " 'prioriti': 28,\n",
       " 'pleas': 28,\n",
       " 'detect': 28,\n",
       " 'young': 28,\n",
       " 'without': 28,\n",
       " 'immun': 28,\n",
       " 'alreadi': 28,\n",
       " 'situat': 28,\n",
       " 'studi': 28,\n",
       " 'sid': 28,\n",
       " 'role': 27,\n",
       " 'treat': 27,\n",
       " 'step': 27,\n",
       " 'honour': 27,\n",
       " 'stay': 27,\n",
       " 'whowpro': 27,\n",
       " 'friend': 27,\n",
       " 'show': 27,\n",
       " 'offer': 27,\n",
       " 'identifi': 27,\n",
       " '#ebola': 27,\n",
       " 'medicin': 27,\n",
       " 'issu': 27,\n",
       " 'spread': 27,\n",
       " '40': 27,\n",
       " 'local': 27,\n",
       " 'presid': 27,\n",
       " 'lifesav': 27,\n",
       " 'fulli': 26,\n",
       " 'booster': 26,\n",
       " 'cest': 26,\n",
       " 'could': 26,\n",
       " 'babi': 26,\n",
       " '#letstalk': 26,\n",
       " 'immedi': 26,\n",
       " '#suicid': 26,\n",
       " 'train': 26,\n",
       " 'univers': 26,\n",
       " 'acceler': 26,\n",
       " 'toward': 26,\n",
       " 'distribut': 26,\n",
       " 'free': 26,\n",
       " 'initi': 26,\n",
       " 'solidar': 26,\n",
       " 'guidelin': 26,\n",
       " 'jensspahn': 26,\n",
       " 'sector': 25,\n",
       " 'across': 25,\n",
       " 'advanc': 25,\n",
       " 'power': 25,\n",
       " 'mean': 25,\n",
       " 'activ': 25,\n",
       " 'understand': 25,\n",
       " 'heart': 25,\n",
       " 'visit': 25,\n",
       " 'requir': 25,\n",
       " 'potenti': 25,\n",
       " '#generationequ': 25,\n",
       " 'humanitarian': 24,\n",
       " 'air': 24,\n",
       " 'contribut': 24,\n",
       " 'cancer': 24,\n",
       " 'long': 24,\n",
       " 'threat': 24,\n",
       " 'administ': 24,\n",
       " 'urg': 24,\n",
       " 'platform': 24,\n",
       " 'origin': 24,\n",
       " 'deploy': 24,\n",
       " 'vital': 23,\n",
       " 'kill': 23,\n",
       " 'place': 23,\n",
       " 'experi': 23,\n",
       " 'abl': 23,\n",
       " 'cours': 23,\n",
       " 'differ': 23,\n",
       " 'seriou': 23,\n",
       " 'best': 23,\n",
       " 'earli': 23,\n",
       " 'remark': 23,\n",
       " 'summit': 23,\n",
       " 'gavi': 23,\n",
       " 'back': 23,\n",
       " 'drmikeryan': 23,\n",
       " 'globalgoalsun': 23,\n",
       " 'low': 23,\n",
       " 'establish': 23,\n",
       " 'prepar': 23,\n",
       " 'g20': 23,\n",
       " 'play': 22,\n",
       " 'result': 22,\n",
       " '5⃣': 22,\n",
       " 'girl': 22,\n",
       " 'whoeurop': 22,\n",
       " 'everywher': 22,\n",
       " 'sign': 22,\n",
       " 'leav': 22,\n",
       " 'go': 22,\n",
       " '5': 22,\n",
       " 'press': 22,\n",
       " 'financi': 22,\n",
       " 'collabor': 22,\n",
       " 'act': 22,\n",
       " '#lebanon': 22,\n",
       " 'clean': 22,\n",
       " 'africanunion': 22,\n",
       " 'strong': 21,\n",
       " 'lack': 21,\n",
       " 'thousand': 21,\n",
       " 'among': 21,\n",
       " 'afford': 21,\n",
       " 'equal': 21,\n",
       " 'encourag': 21,\n",
       " 'gap': 21,\n",
       " 'miss': 21,\n",
       " 'centr': 21,\n",
       " 'intellig': 21,\n",
       " 'clear': 21,\n",
       " 'rate': 21,\n",
       " '#actogeth': 21,\n",
       " 'world’': 21,\n",
       " 'addit': 21,\n",
       " 'polit': 21,\n",
       " 'avoid': 21,\n",
       " 'diet': 21,\n",
       " '🤱': 21,\n",
       " '#worldhumanitarianday': 21,\n",
       " 'oxygen': 20,\n",
       " 'chikwei': 20,\n",
       " 'healthcar': 20,\n",
       " 'recogn': 20,\n",
       " 'healthier': 20,\n",
       " 'harm': 20,\n",
       " 'respect': 20,\n",
       " 'decis': 20,\n",
       " '\\u200d': 20,\n",
       " 'experienc': 20,\n",
       " 'suffer': 20,\n",
       " 'strategi': 20,\n",
       " 'partnership': 20,\n",
       " 'coordin': 20,\n",
       " 'adult': 20,\n",
       " 'antonioguterr': 20,\n",
       " 'compani': 20,\n",
       " 'lowincom': 20,\n",
       " 'programm': 20,\n",
       " 'engag': 20,\n",
       " 'noncommunic': 20,\n",
       " '#unga': 20,\n",
       " 'secur': 20,\n",
       " 'workforc': 20,\n",
       " 'countriesdrtedro': 20,\n",
       " 'detail': 20,\n",
       " 'mother': 20,\n",
       " 'cepivaccin': 20,\n",
       " 'breastfe': 20,\n",
       " 'donat': 19,\n",
       " 'surveil': 19,\n",
       " 'twitter': 19,\n",
       " 'limit': 19,\n",
       " 'event': 19,\n",
       " 'disrupt': 19,\n",
       " 'who': 19,\n",
       " '#foodsystem': 19,\n",
       " 'benefit': 19,\n",
       " 'educ': 19,\n",
       " '#depress': 19,\n",
       " 'alon': 19,\n",
       " 'say': 19,\n",
       " 'danger': 19,\n",
       " 'give': 19,\n",
       " 'drug': 19,\n",
       " 'happen': 19,\n",
       " 'roll': 19,\n",
       " 'malaria': 19,\n",
       " 'approach': 19,\n",
       " 'innov': 19,\n",
       " 'collect': 19,\n",
       " '3': 19,\n",
       " 'african': 19,\n",
       " 'produc': 19,\n",
       " 'mask': 19,\n",
       " 'welcom': 19,\n",
       " 'look': 19,\n",
       " 'pathogen': 19,\n",
       " 'safeti': 19,\n",
       " 'jnkengasong': 19,\n",
       " 'songwevera': 19,\n",
       " '#openwho': 18,\n",
       " 'equip': 18,\n",
       " 'facebook': 18,\n",
       " 'linkedin': 18,\n",
       " 'youtub': 18,\n",
       " 'drive': 18,\n",
       " 'onlin': 18,\n",
       " 'enabl': 18,\n",
       " 'special': 18,\n",
       " 'hope': 18,\n",
       " 'la': 18,\n",
       " 'stop': 18,\n",
       " 'common': 18,\n",
       " 'isol': 18,\n",
       " 'symptom': 18,\n",
       " 'solut': 18,\n",
       " 'child': 18,\n",
       " 'water': 18,\n",
       " '2020': 18,\n",
       " 'strateg': 18,\n",
       " 'older': 18,\n",
       " 'rapidli': 18,\n",
       " 'allow': 18,\n",
       " 'half': 18,\n",
       " 'trial': 18,\n",
       " '4': 18,\n",
       " 'due': 18,\n",
       " 'blood': 18,\n",
       " 'forward': 18,\n",
       " 'director': 18,\n",
       " 'small': 18,\n",
       " 'cyrilramaphosa': 18,\n",
       " 'often': 18,\n",
       " 'wait': 18,\n",
       " '#youthday': 18,\n",
       " 'within': 17,\n",
       " 'whosearo': 17,\n",
       " 'enough': 17,\n",
       " 'gener': 17,\n",
       " 'safer': 17,\n",
       " 'committe': 17,\n",
       " 'mobil': 17,\n",
       " 'consid': 17,\n",
       " 'campaign': 17,\n",
       " 'remind': 17,\n",
       " 'survivor': 17,\n",
       " 'behind': 17,\n",
       " 'civil': 17,\n",
       " 'fulfil': 17,\n",
       " '15': 17,\n",
       " 'elimin': 17,\n",
       " 'list': 17,\n",
       " 'independ': 17,\n",
       " '#wearamask': 17,\n",
       " 'home': 17,\n",
       " 'lower': 17,\n",
       " 'exploit': 17,\n",
       " 'knowledg': 17,\n",
       " '’s': 17,\n",
       " '#covidsummit': 17,\n",
       " 'evid': 17,\n",
       " 'chancellor': 17,\n",
       " 'hepat': 17,\n",
       " 'africacdc': 17,\n",
       " 'medspatentpool': 17,\n",
       " 'mrcza': 17,\n",
       " '#hepat': 17,\n",
       " 'island': 17,\n",
       " 'gender': 16,\n",
       " 'whether': 16,\n",
       " 'recoveri': 16,\n",
       " 'larg': 16,\n",
       " 'forum': 16,\n",
       " 'comprehens': 16,\n",
       " 'move': 16,\n",
       " 'we’r': 16,\n",
       " 'don’t': 16,\n",
       " 'de': 16,\n",
       " 'contact': 16,\n",
       " 'twitterspac': 16,\n",
       " 'abus': 16,\n",
       " 'factor': 16,\n",
       " 'posit': 16,\n",
       " 'polici': 16,\n",
       " 'aim': 16,\n",
       " '#dementia': 16,\n",
       " 'becom': 16,\n",
       " '#sciencein5': 16,\n",
       " 'highincom': 16,\n",
       " 'implement': 16,\n",
       " 'monitor': 16,\n",
       " 'process': 16,\n",
       " 'occur': 16,\n",
       " 'avat': 16,\n",
       " 'knowhow': 16,\n",
       " 'scienc': 16,\n",
       " 'ago': 16,\n",
       " '30': 16,\n",
       " 'past': 16,\n",
       " '#mening': 16,\n",
       " 'serv': 16,\n",
       " 'faster': 16,\n",
       " 'tobacco': 16,\n",
       " 'vaccinesdrtedro': 16,\n",
       " 'stronger': 15,\n",
       " 'pollut': 15,\n",
       " 'extrem': 15,\n",
       " 'wave': 15,\n",
       " 'particip': 15,\n",
       " '6⃣': 15,\n",
       " '4⃣': 15,\n",
       " 'thing': 15,\n",
       " 'grow': 15,\n",
       " 'en': 15,\n",
       " '🤰': 15,\n",
       " 'think': 15,\n",
       " 'suicid': 15,\n",
       " 'job': 15,\n",
       " 'combin': 15,\n",
       " 'dont': 15,\n",
       " 'announc': 15,\n",
       " 'done': 15,\n",
       " 'acut': 15,\n",
       " 'guidanc': 15,\n",
       " 'three': 15,\n",
       " 'priorit': 15,\n",
       " 'economi': 15,\n",
       " 'record': 15,\n",
       " 'scientif': 15,\n",
       " 'ever': 15,\n",
       " 'want': 15,\n",
       " 'that’': 15,\n",
       " 'base': 15,\n",
       " 'matern': 15,\n",
       " '90': 15,\n",
       " 'rang': 15,\n",
       " 'expand': 15,\n",
       " 'tomorrow': 15,\n",
       " '#hypertens': 15,\n",
       " '🧠': 15,\n",
       " 'staff': 15,\n",
       " 'individu': 15,\n",
       " 'skill': 15,\n",
       " 'offic': 15,\n",
       " 'recent': 15,\n",
       " 'merkel': 15,\n",
       " 'breastfeed': 15,\n",
       " '#bahrain': 15,\n",
       " 'resili': 14,\n",
       " '#africa': 14,\n",
       " 'whoafghanistan': 14,\n",
       " '#climateact': 14,\n",
       " 'focu': 14,\n",
       " 'natur': 14,\n",
       " 'climat': 14,\n",
       " 'nearli': 14,\n",
       " 'transform': 14,\n",
       " 'barrier': 14,\n",
       " '\\u200d\\u200d': 14,\n",
       " 'problem': 14,\n",
       " 'eat': 14,\n",
       " 'advoc': 14,\n",
       " '#drc': 14,\n",
       " 'met': 14,\n",
       " 'primari': 14,\n",
       " 'donor': 14,\n",
       " 'alway': 14,\n",
       " 'assess': 14,\n",
       " 'standard': 14,\n",
       " 'sure': 14,\n",
       " 'estim': 14,\n",
       " 'advisori': 14,\n",
       " 'attend': 14,\n",
       " 'hand': 14,\n",
       " 'pressur': 14,\n",
       " 'latest': 14,\n",
       " 'commiss': 14,\n",
       " 'youth': 14,\n",
       " 'g7': 14,\n",
       " 'quickli': 14,\n",
       " 'schwartlanderb': 14,\n",
       " 'wear': 14,\n",
       " 'sourc': 13,\n",
       " 'privat': 13,\n",
       " 'although': 13,\n",
       " 'repres': 13,\n",
       " 'ahead': 13,\n",
       " 'worldwid': 13,\n",
       " 'digit': 13,\n",
       " '…': 13,\n",
       " 'rememb': 13,\n",
       " 'feel': 13,\n",
       " 'never': 13,\n",
       " 'tip': 13,\n",
       " 'institut': 13,\n",
       " 'transfer': 13,\n",
       " 'technic': 13,\n",
       " 'said': 13,\n",
       " 'middl': 13,\n",
       " 'approv': 13,\n",
       " '#endmalaria': 13,\n",
       " 'track': 13,\n",
       " '20': 13,\n",
       " 'widespread': 13,\n",
       " 'goal': 13,\n",
       " 'unnewscentr': 13,\n",
       " 'doubl': 13,\n",
       " '50': 13,\n",
       " 'victim': 13,\n",
       " 'screen': 13,\n",
       " 'present': 13,\n",
       " 'moment': 13,\n",
       " 'excel': 13,\n",
       " 'contracept': 13,\n",
       " 'agre': 13,\n",
       " 'project': 13,\n",
       " 'longterm': 13,\n",
       " 'forc': 13,\n",
       " 'joint': 13,\n",
       " '#marburg': 13,\n",
       " '#worldsuicidepreventionday': 13,\n",
       " 'equiti': 12,\n",
       " 'soon': 12,\n",
       " 'school': 12,\n",
       " 'a…': 12,\n",
       " 'pregnanc': 12,\n",
       " 'therapeut': 12,\n",
       " 'idea': 12,\n",
       " 'news': 12,\n",
       " 'love': 12,\n",
       " 'signific': 12,\n",
       " 'plu': 12,\n",
       " 'hear': 12,\n",
       " 'trust': 12,\n",
       " 'later': 12,\n",
       " '#palliativecar': 12,\n",
       " 'highlight': 12,\n",
       " 'investig': 12,\n",
       " 'colleagu': 12,\n",
       " 'author': 12,\n",
       " 'updat': 12,\n",
       " 'atrisk': 12,\n",
       " 'incom': 12,\n",
       " 'diagnost': 12,\n",
       " 'form': 12,\n",
       " '#healthdata': 12,\n",
       " '#tigray': 12,\n",
       " 'advic': 12,\n",
       " '2030': 12,\n",
       " 'highest': 12,\n",
       " 'damag': 12,\n",
       " 'expect': 12,\n",
       " 'mechan': 12,\n",
       " 'wto': 12,\n",
       " '100': 12,\n",
       " 'lyon': 12,\n",
       " 'gain': 12,\n",
       " 'second': 12,\n",
       " 'transmit': 12,\n",
       " 'hiv': 12,\n",
       " 'discrimin': 12,\n",
       " 'turn': 12,\n",
       " 'malnutrit': 12,\n",
       " 'multipl': 12,\n",
       " 'dementia': 12,\n",
       " 'injuri': 12,\n",
       " 'mrna': 12,\n",
       " 'sago': 12,\n",
       " 'pandemicdrtedro': 12,\n",
       " 'earthquak': 12,\n",
       " 'shown': 11,\n",
       " 'congratul': 11,\n",
       " '#cervicalcanc': 11,\n",
       " 'tune': 11,\n",
       " 'fuel': 11,\n",
       " 'listen': 11,\n",
       " 'adolesc': 11,\n",
       " 'difficult': 11,\n",
       " 'watch': 11,\n",
       " 'profession': 11,\n",
       " 'lost': 11,\n",
       " 'chronic': 11,\n",
       " 'pain': 11,\n",
       " 'here': 11,\n",
       " 'octob': 11,\n",
       " 'enjoy': 11,\n",
       " 'particularli': 11,\n",
       " 'cant': 11,\n",
       " 'alloc': 11,\n",
       " 'middleincom': 11,\n",
       " 'fall': 11,\n",
       " 'pledg': 11,\n",
       " 'guid': 11,\n",
       " '11': 11,\n",
       " 'yeardrtedro': 11,\n",
       " 'katelobrien': 11,\n",
       " 'consist': 11,\n",
       " 'exist': 11,\n",
       " '2019': 11,\n",
       " 'newborn': 11,\n",
       " 't…': 11,\n",
       " 'prime': 11,\n",
       " 'tackl': 11,\n",
       " 'diabet': 11,\n",
       " 'involv': 11,\n",
       " 'expos': 11,\n",
       " 'consequ': 11,\n",
       " 'employ': 11,\n",
       " '#franc': 11,\n",
       " 'host': 11,\n",
       " 'game': 11,\n",
       " 'cooper': 11,\n",
       " 'framework': 11,\n",
       " 'option': 11,\n",
       " '#beatncd': 11,\n",
       " 'unnewsarab': 11,\n",
       " 'term': 11,\n",
       " 'milk': 11,\n",
       " '#sepsi': 11,\n",
       " 'shot': 11,\n",
       " 'anoth': 11,\n",
       " 'berlin': 11,\n",
       " 'worldbank': 11,\n",
       " 'regul': 11,\n",
       " 'contain': 11,\n",
       " 'let’': 11,\n",
       " 'ecigarett': 11,\n",
       " '🤱🤱🤱🤱🤱': 11,\n",
       " '#actforequ': 11,\n",
       " 'disast': 10,\n",
       " 'readi': 10,\n",
       " 'waiv': 10,\n",
       " 'total': 10,\n",
       " 'head': 10,\n",
       " 'breath': 10,\n",
       " 'unhealthi': 10,\n",
       " 'polio': 10,\n",
       " 'flood': 10,\n",
       " 'disord': 10,\n",
       " 'despit': 10,\n",
       " 'your': 10,\n",
       " 'chat': 10,\n",
       " 'check': 10,\n",
       " 'burden': 10,\n",
       " 'yesterday': 10,\n",
       " 'ministri': 10,\n",
       " 'true': 10,\n",
       " 'greater': 10,\n",
       " 'cannot': 10,\n",
       " 'matter': 10,\n",
       " 'i’m': 10,\n",
       " 'sick': 10,\n",
       " '#vaccineswork': 10,\n",
       " '2000': 10,\n",
       " 'breast': 10,\n",
       " 'brain': 10,\n",
       " '60': 10,\n",
       " 'cost': 10,\n",
       " 'parent': 10,\n",
       " 'higher': 10,\n",
       " 'covax': 10,\n",
       " 'divers': 10,\n",
       " 'assist': 10,\n",
       " '#primaryhealthcar': 10,\n",
       " 'success': 10,\n",
       " 'stress': 10,\n",
       " 'seri': 10,\n",
       " 'moetitshidi': 10,\n",
       " 'choos': 10,\n",
       " 'defeat': 10,\n",
       " 'taken': 10,\n",
       " 'anywher': 10,\n",
       " 'bodi': 10,\n",
       " 'top': 10,\n",
       " 'reflect': 10,\n",
       " 'devast': 10,\n",
       " 'mark': 10,\n",
       " 'novel': 10,\n",
       " '#hiv': 10,\n",
       " 'statement': 10,\n",
       " 'exampl': 10,\n",
       " 'shortag': 10,\n",
       " 'empow': 10,\n",
       " 'grate': 10,\n",
       " 'unit': 10,\n",
       " 'responsedrtedro': 10,\n",
       " 'rise': 10,\n",
       " '#haiti': 10,\n",
       " '#youthlead': 10,\n",
       " 'intellectu': 9,\n",
       " 'properti': 9,\n",
       " 'to…': 9,\n",
       " 'diagnosi': 9,\n",
       " 'august': 9,\n",
       " 'celebr': 9,\n",
       " 'relat': 9,\n",
       " 'relationship': 9,\n",
       " 'significantli': 9,\n",
       " 'caregiv': 9,\n",
       " 'intervent': 9,\n",
       " 'can’t': 9,\n",
       " 'speak': 9,\n",
       " 'loneli': 9,\n",
       " '#': 9,\n",
       " 'warn': 9,\n",
       " 'valu': 9,\n",
       " 'express': 9,\n",
       " 'anyon': 9,\n",
       " 'loss': 9,\n",
       " 'palli': 9,\n",
       " 'failur': 9,\n",
       " 'great': 9,\n",
       " 'fire': 9,\n",
       " '6': 9,\n",
       " 'outlin': 9,\n",
       " 'mid2022': 9,\n",
       " 'appropri': 9,\n",
       " 'publish': 9,\n",
       " 'known': 9,\n",
       " 'contract': 9,\n",
       " 'given': 9,\n",
       " 'stand': 9,\n",
       " 'confer': 9,\n",
       " 'travel': 9,\n",
       " 'africadrtedro': 9,\n",
       " 'worlddrtedro': 9,\n",
       " 'ongo': 9,\n",
       " '9': 9,\n",
       " 'birth': 9,\n",
       " 'reproduct': 9,\n",
       " 'moratorium': 9,\n",
       " 'prematur': 9,\n",
       " 'hold': 9,\n",
       " '#defeatmening': 9,\n",
       " 'cardiovascular': 9,\n",
       " 'account': 9,\n",
       " 'type': 9,\n",
       " 'vision': 9,\n",
       " 'incid': 9,\n",
       " 'voic': 9,\n",
       " 'deepli': 9,\n",
       " 'certain': 9,\n",
       " 'confirm': 9,\n",
       " 'evolv': 9,\n",
       " 'appli': 9,\n",
       " 'ground': 9,\n",
       " 'femal': 9,\n",
       " 'wide': 9,\n",
       " 'procur': 9,\n",
       " 'pay': 9,\n",
       " 'basic': 9,\n",
       " 'left': 9,\n",
       " 'facilit': 9,\n",
       " 'mitig': 9,\n",
       " 'hard': 9,\n",
       " 'choic': 9,\n",
       " 'lesson': 9,\n",
       " 'main': 9,\n",
       " 'third': 9,\n",
       " 'glad': 9,\n",
       " '#worldpatientsafetyday': 9,\n",
       " 'doctorsoumya': 9,\n",
       " 'afreximbank': 9,\n",
       " 'everyth': 9,\n",
       " 'ifpma': 9,\n",
       " 'imfnew': 9,\n",
       " 'compar': 9,\n",
       " 'ernasolberg': 9,\n",
       " 'nakufoaddo': 9,\n",
       " 'assembl': 9,\n",
       " 'panel': 9,\n",
       " 'sport': 9,\n",
       " 'rollout': 9,\n",
       " 'stage': 9,\n",
       " 'blocker': 9,\n",
       " 'b': 9,\n",
       " 'fragil': 8,\n",
       " 'mileston': 8,\n",
       " 'appreci': 8,\n",
       " 'arriv': 8,\n",
       " 'road': 8,\n",
       " '80': 8,\n",
       " 'hpv': 8,\n",
       " 'threaten': 8,\n",
       " 'integr': 8,\n",
       " 'decad': 8,\n",
       " 'rel': 8,\n",
       " 'realiti': 8,\n",
       " 'depress': 8,\n",
       " 'away': 8,\n",
       " 'stori': 8,\n",
       " 'led': 8,\n",
       " 'messag': 8,\n",
       " 'thought': 8,\n",
       " 'deal': 8,\n",
       " 'tri': 8,\n",
       " 'regular': 8,\n",
       " 'connect': 8,\n",
       " 'concert': 8,\n",
       " 'nurs': 8,\n",
       " 'declar': 8,\n",
       " 'in…': 8,\n",
       " 'afghan': 8,\n",
       " 'oper': 8,\n",
       " 'tell': 8,\n",
       " 'post': 8,\n",
       " '7': 8,\n",
       " 'crucial': 8,\n",
       " 'swap': 8,\n",
       " 'delight': 8,\n",
       " 'minut': 8,\n",
       " 'globalfund': 8,\n",
       " 'began': 8,\n",
       " 'incred': 8,\n",
       " 'foreign': 8,\n",
       " '#aworld4allag': 8,\n",
       " 'insecur': 8,\n",
       " 'oral': 8,\n",
       " 'america': 8,\n",
       " 'beyond': 8,\n",
       " ...}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for the queries\n",
    "top_10 = {}\n",
    "for i in index:\n",
    "    top_10[i]=len(index[i])\n",
    "top_10 = dict(sorted(top_10.items(), reverse=True, key=lambda item: item[1]))\n",
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Sample of {} results out of {} for the query ' covid vaccine '\n",
      "\n",
      "tweet_id= 2048 - tweet: @DrTedros \"The global gap in #COVID19 vaccine supply is hugely uneven and inequitable. Some countries and regions are actually ordering millions of booster doses, before other countries have had supplies to vaccinate their #healthworkers and most vulnerable\"-@DrTedros #VaccinEquity\n",
      "\n",
      "tweet_id= 2050 - tweet: @DrTedros \"It is definitely worse in places that have very few vaccines but the #COVID19 pandemic is not over anywhere. The current collective strategy reminds me of a fire fighting team taking on a forest blaze\"-@DrTedros https://t.co/qFijG1cZj0\n",
      "\n",
      "tweet_id= 4 - tweet: RT @WHOAFRO: Congratulations Algeria🇩🇿!\n",
      "\n",
      "#Algeria is the 16th country in #Africa to reach the milestone of fully vaccinating 10% of its pop…\n",
      "\n",
      "tweet_id= 2053 - tweet: @DrTedros \"Vaccines have never been the way out of this crisis on their own but this current wave is demonstrating again just what a powerful tool they are to battle back against this virus\"-@DrTedros #COVID19 #VaccinEquity\n",
      "\n",
      "tweet_id= 7 - tweet: RT @DrTedros: We must appreciate the role private sector has played in the #COVID19 response and the development of vaccines in the shortes…\n",
      "\n",
      "tweet_id= 8 - tweet: RT @DrTedros: Humanity is failing miserably with vaccine injustice. Debating whether intellectual property rights should be waived in such…\n",
      "\n",
      "tweet_id= 2056 - tweet: @DrTedros \"In countries with low vaccine coverage, the situation is particularly bad. Delta and other highly transmissible variants are driving catastrophic waves of #COVID19 cases, which are translating into high numbers of hospitalisations and death\"-@DrTedros \n",
      "https://t.co/AAY6DONlAx\n",
      "\n",
      "tweet_id= 2059 - tweet: @DrTedros \"In places with high vaccination coverage, Delta is spreading quickly; especially infecting unprotected and vulnerable people and steadily putting pressure back on health systems\"-@DrTedros #COVID19 \n",
      "\n",
      "https://t.co/AAY6DONlAx\n",
      "\n",
      "tweet_id= 23 - tweet: Join us celebrate the world-changing legacy of Henrietta Lacks, whose #HeLaCells enable medical breakthroughs - from HPV, polio vaccines to #COVID19 research. \n",
      "Join us &amp; the @LacksFamily honouring her legacy &amp; calling for #HealthEquity around the world.\n",
      "▶️ https://t.co/ZrWkKGkhXa https://t.co/SafvwL3KmF\n",
      "\n",
      "tweet_id= 2073 - tweet: RT @DrTedros: As the @g20org communique says, vaccination against #COVID19 is a public good, doses must be equitably shared via @ACTAcceler…\n",
      "\n",
      "\n",
      "======================\n",
      "Sample of {} results out of {} for the query ' global pandemic '\n",
      "\n",
      "tweet_id= 2048 - tweet: @DrTedros \"The global gap in #COVID19 vaccine supply is hugely uneven and inequitable. Some countries and regions are actually ordering millions of booster doses, before other countries have had supplies to vaccinate their #healthworkers and most vulnerable\"-@DrTedros #VaccinEquity\n",
      "\n",
      "tweet_id= 2049 - tweet: @DrTedros \"Hosing down part of it might reduce the flames in one area but while it’s smouldering anywhere, sparks will eventually travel &amp; grow again into a roaring furnace. The 🌍 should battle together to put out the #COVID19 pandemic inferno everywhere\"-@DrTedros\n",
      "https://t.co/tSotlnVYuV\n",
      "\n",
      "tweet_id= 2050 - tweet: @DrTedros \"It is definitely worse in places that have very few vaccines but the #COVID19 pandemic is not over anywhere. The current collective strategy reminds me of a fire fighting team taking on a forest blaze\"-@DrTedros https://t.co/qFijG1cZj0\n",
      "\n",
      "tweet_id= 2051 - tweet: @DrTedros \"My message today is that we are experiencing a worsening public health emergency that further threatens lives, livelihoods and a sound global economic recovery\"-@DrTedros #COVID19 \n",
      "\n",
      "https://t.co/IkT9UHqcqW\n",
      "\n",
      "tweet_id= 12 - tweet: RT @DrTedros: Donations aren't enough to deliver on #VaccinEquity and end the #COVID19 pandemic. We need stronger leadership to ramp up the…\n",
      "\n",
      "tweet_id= 2060 - tweet: @DrTedros \"Not everywhere is taking the same hit though, we’re in the midst of a growing two-track #COVID19 pandemic where the haves and have-nots within and between countries are increasingly divergent\"-@DrTedros \n",
      "\n",
      "https://t.co/RkOh67W68s\n",
      "\n",
      "tweet_id= 2062 - tweet: @DrTedros \"Last week marked the 4th consecutive week of increasing cases of #COVID19 globally, with increases recorded in all but one of WHO’s six regions. And after 10 weeks of declines, deaths are increasing again\"-@DrTedros\n",
      "\n",
      "tweet_id= 15 - tweet: LIVE with @DrTedros: Ending the #COVID19 pandemic: road to an inclusive recovery. #VaccinEquity https://t.co/q9x4v3ixBl\n",
      "\n",
      "tweet_id= 16 - tweet: RT @DrTedros: Was good to meet @Chikwe_I, who will soon head our work on health emergency surveillance, incl the @WHO Hub for Pandemic &amp; Ep…\n",
      "\n",
      "tweet_id= 2066 - tweet: Two 🆕 reports provide the first global recommendations to help establish human genome 🧬 editing as a tool for public health, with an emphasis on safety, effectiveness and ethics. \n",
      "\n",
      "👉 https://t.co/w32QhMXSXi https://t.co/Bto9SF6NzW\n",
      "\n",
      "\n",
      "======================\n",
      "Sample of {} results out of {} for the query ' world disease '\n",
      "\n",
      "tweet_id= 513 - tweet: 🌎🌍🌏 progress in provision of #tuberculosis preventive treatment lags behind with only 1 in 5 of the 30 million people targeted for access by 2022 reached.\n",
      " \n",
      "Let's expand access to TB preventive treatment that can stop the infection from developing into disease. Let's #EndTB! https://t.co/DZtfqHgZKg\n",
      "\n",
      "tweet_id= 2052 - tweet: @DrTedros \"Delta is now in more than 104 countries and we expect it to soon be the dominant #COVID19 strain circulating worldwide. The world is watching in real time as the COVID-19 virus continues to change and become more transmissible\"-@DrTedros\n",
      "\n",
      "tweet_id= 524 - tweet: Each day, over 4000 people die from #tuberculosis &amp; close to 30000 people fall ill with this preventable &amp; curable disease.\n",
      "\n",
      "Even as we battle #COVID19, we must not ease up in the fight to #EndTB, which remains 1 of the 🌍’s deadliest infectious killers.\n",
      "👉https://t.co/UsqIlQ307L https://t.co/3iWjSuTP5V\n",
      "\n",
      "tweet_id= 1549 - tweet: #Depression affects more than 260 million people in the world while around half of all #mentalhealth conditions start by age 14.\n",
      "\n",
      "#Suicide is the fourth leading cause of death in young people aged 15-29.\n",
      "\n",
      "⚽️ https://t.co/VMZxBomqlZ #ReachOut \n",
      "\n",
      "https://t.co/DElF0AG8Rn\n",
      "\n",
      "tweet_id= 2061 - tweet: @DrTedros \"We continue to hear reports from all regions of the world about hospitals reaching capacity. The Delta variant is ripping around the world at a scorching pace, driving a new spike in #COVID19 cases and death\"-@DrTedros \n",
      "\n",
      "https://t.co/0JJXTNPIP5\n",
      "\n",
      "tweet_id= 1037 - tweet: WHO releases new compendium of innovative health technologies that can be used in low-resource settings for #COVID19 and other priority diseases.\n",
      "▶️ https://t.co/SwVaGP6Hr5 https://t.co/saYg2J5xNK\n",
      "\n",
      "tweet_id= 20 - tweet: Ahead of #COP26, 3⃣0⃣0⃣ organizations representing at least 4⃣5⃣ million #HealthWorkers worldwide are calling for world leaders to step up #ClimateAction by limiting global warming to 1.5°C, and put health at the center of the plans.\n",
      " \n",
      "Read more 👉 https://t.co/WXMdMgPSWv https://t.co/iax40Auf8d\n",
      "\n",
      "tweet_id= 533 - tweet: Several countries pledged additional dose donations to be made available to countries around the world 🌍:\n",
      "🇺🇸 USA 500 M doses\n",
      "🇸🇪 Sweden SEK 2.1 billion in cash donations &amp; doses\n",
      "🇮🇹 Italy 30 M doses.\n",
      "🇪🇸 Spain 7.5M doses\n",
      "🇩🇰 Denmark 6M \n",
      "🇯🇵 Japan 60 M doses\n",
      "\n",
      "https://t.co/mEkx2smDgl\n",
      "\n",
      "tweet_id= 23 - tweet: Join us celebrate the world-changing legacy of Henrietta Lacks, whose #HeLaCells enable medical breakthroughs - from HPV, polio vaccines to #COVID19 research. \n",
      "Join us &amp; the @LacksFamily honouring her legacy &amp; calling for #HealthEquity around the world.\n",
      "▶️ https://t.co/ZrWkKGkhXa https://t.co/SafvwL3KmF\n",
      "\n",
      "tweet_id= 2071 - tweet: RT @DrTedros: Thank you, @BTS_twt, for including sign language in your #PermissiontoDance music video.\n",
      "As 1.5 billion people in the world a…\n",
      "\n",
      "\n",
      "======================\n",
      "Sample of {} results out of {} for the query ' emergency call '\n",
      "\n",
      "tweet_id= 1 - tweet: It's International Day for Disaster Risk Reduction\n",
      "\n",
      "#OpenWHO has launched a multi-tiered core curriculum to help equip you with the competencies needed to work within public health emergency response.\n",
      "\n",
      "Start learning today &amp; be #Ready4Response:\n",
      "👉 https://t.co/hBFFOF0xKL https://t.co/fgZY22RWuS\n",
      "\n",
      "tweet_id= 2 - tweet: #COVID19 has shown how health emergencies and disasters affect entire communities – especially those with weak health systems, and vulnerable populations like migrants, indigenous peoples, and those living in fragile humanitarian conditions. https://t.co/jpUQpnu0V1\n",
      "\n",
      "tweet_id= 3 - tweet: It's International Day for Disaster Risk Reduction\n",
      " \n",
      "To better respond to emergencies countries must:\n",
      "✅ invest in health care systems\n",
      "✅ achieve gender equity\n",
      "✅ protect marginalised groups\n",
      "✅ ensure ready &amp; equitable access to supplies\n",
      " \n",
      "A strong &amp; resilient health system is 🔑 https://t.co/5NALyjIymp\n",
      "\n",
      "tweet_id= 2051 - tweet: @DrTedros \"My message today is that we are experiencing a worsening public health emergency that further threatens lives, livelihoods and a sound global economic recovery\"-@DrTedros #COVID19 \n",
      "\n",
      "https://t.co/IkT9UHqcqW\n",
      "\n",
      "tweet_id= 1030 - tweet: 🇩🇪 is a 🔑supporter of health emergency preparedness and response &amp; the top donor to the WHO Contingency Fund for Emergencies, contributing more than US$68 million of flexible funding since 2015, supporting WHO's response to more than 120 emergencies.\n",
      "\n",
      "https://t.co/o07XThTks0\n",
      "\n",
      "tweet_id= 1033 - tweet: WHO is proud to partner with #Germany🇩🇪, a staunch advocate for 🌍 health &amp; WHO’s largest donor, supporting our work to:\n",
      "✅strengthen global health\n",
      "✅combat antimicrobial resistance\n",
      "✅achieve universal health coverage\n",
      "✅respond to emergencies\n",
      "✅protect the vulnerable https://t.co/xgv7TTfI8p\n",
      "\n",
      "tweet_id= 16 - tweet: RT @DrTedros: Was good to meet @Chikwe_I, who will soon head our work on health emergency surveillance, incl the @WHO Hub for Pandemic &amp; Ep…\n",
      "\n",
      "tweet_id= 2065 - tweet: The 🆕 forward-looking reports provide advice on human genome 🧬 editing, a rapidly emerging science that could provide potential benefits including:\n",
      "🔸 faster and more accurate diagnosis\n",
      "🔸 more targeted treatments\n",
      "🔸 prevention of genetic disorders \n",
      "\n",
      "👉 https://t.co/Dp301yAMDB https://t.co/pUNqY9pSa3\n",
      "\n",
      "tweet_id= 19 - tweet: ☎️ Will you take the #ClimateAction call❓\n",
      " \n",
      "Countries must set ambitious commitments if they are to sustain a healthy &amp; green recovery from the pandemic.\n",
      " \n",
      "Here are WHO’s 🔟 calls to assure sustained recovery from #COVID19 👉 https://t.co/WXMdMgPSWv\n",
      "https://t.co/aSBosI6lmZ\n",
      "\n",
      "tweet_id= 20 - tweet: Ahead of #COP26, 3⃣0⃣0⃣ organizations representing at least 4⃣5⃣ million #HealthWorkers worldwide are calling for world leaders to step up #ClimateAction by limiting global warming to 1.5°C, and put health at the center of the plans.\n",
      " \n",
      "Read more 👉 https://t.co/WXMdMgPSWv https://t.co/iax40Auf8d\n",
      "\n",
      "\n",
      "======================\n",
      "Sample of {} results out of {} for the query ' countries risk '\n",
      "\n",
      "tweet_id= 2048 - tweet: @DrTedros \"The global gap in #COVID19 vaccine supply is hugely uneven and inequitable. Some countries and regions are actually ordering millions of booster doses, before other countries have had supplies to vaccinate their #healthworkers and most vulnerable\"-@DrTedros #VaccinEquity\n",
      "\n",
      "tweet_id= 1 - tweet: It's International Day for Disaster Risk Reduction\n",
      "\n",
      "#OpenWHO has launched a multi-tiered core curriculum to help equip you with the competencies needed to work within public health emergency response.\n",
      "\n",
      "Start learning today &amp; be #Ready4Response:\n",
      "👉 https://t.co/hBFFOF0xKL https://t.co/fgZY22RWuS\n",
      "\n",
      "tweet_id= 3 - tweet: It's International Day for Disaster Risk Reduction\n",
      " \n",
      "To better respond to emergencies countries must:\n",
      "✅ invest in health care systems\n",
      "✅ achieve gender equity\n",
      "✅ protect marginalised groups\n",
      "✅ ensure ready &amp; equitable access to supplies\n",
      " \n",
      "A strong &amp; resilient health system is 🔑 https://t.co/5NALyjIymp\n",
      "\n",
      "tweet_id= 4 - tweet: RT @WHOAFRO: Congratulations Algeria🇩🇿!\n",
      "\n",
      "#Algeria is the 16th country in #Africa to reach the milestone of fully vaccinating 10% of its pop…\n",
      "\n",
      "tweet_id= 2052 - tweet: @DrTedros \"Delta is now in more than 104 countries and we expect it to soon be the dominant #COVID19 strain circulating worldwide. The world is watching in real time as the COVID-19 virus continues to change and become more transmissible\"-@DrTedros\n",
      "\n",
      "tweet_id= 2054 - tweet: @DrTedros \"Particularly in low-income countries, exhausted #healthworkers are battling to save lives in the midst of shortages of personal protective equipment, oxygen and treatments\"-@DrTedros #COVID19\n",
      "\n",
      "tweet_id= 2055 - tweet: @DrTedros \"Even countries that successfully managed to ward off the early waves of the #COVID19 virus, through public health measures alone, are now in the midst of devastating outbreaks\"-@DrTedros\n",
      "\n",
      "tweet_id= 2056 - tweet: @DrTedros \"In countries with low vaccine coverage, the situation is particularly bad. Delta and other highly transmissible variants are driving catastrophic waves of #COVID19 cases, which are translating into high numbers of hospitalisations and death\"-@DrTedros \n",
      "https://t.co/AAY6DONlAx\n",
      "\n",
      "tweet_id= 2057 - tweet: @DrTedros \"As countries lift public health and social measures, they must consider the impact on #healthworkers and health systems\"-@DrTedros #COVID19 \n",
      "\n",
      "https://t.co/PdlUzMzbTl\n",
      "\n",
      "tweet_id= 1027 - tweet: 🆕study undertaken in 🇰🇿, 🇰🇬, 🇹🇯 &amp; 🇺🇦 shows that training people most likely to witness an #opioid overdose &amp; providing them with the drug naloxone, has the potential to significantly ↘️ the number of deaths in these countries if scaled up nationwide.\n",
      " https://t.co/ATU75VtvQi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries = ['covid vaccine', 'global pandemic', 'world disease', 'emergency call','countries risk']\n",
    "for i in queries:\n",
    "    docs = search(i, index)\n",
    "    top = 10\n",
    "\n",
    "    print(\"\\n======================\\nSample of {} results out of {} for the query '\", i,\"'\\n\".format(top, len(docs)))\n",
    "    for d_id in docs[:top]:\n",
    "        print(\"tweet_id= {} - tweet: {}\\n\".format(d_id, title_index[d_id]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
