{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRWA Final Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to build a search engine implementing different indexing and ranking algorithms. This will be done using a file containing a set of tweets from the World Health Organization (@WHO).\n",
    "\n",
    "It will be divided in four parts:\n",
    "\n",
    "    1) Text processing\n",
    "    2) Indexing and ranking\n",
    "    3) Evaluation \n",
    "    4) User Interface and Web analytics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Students Group 9:\n",
    "- Mireia Beltran (U161808)\n",
    "- Cisco Orteu (U162354)\n",
    "- Laura Casanovas (U161832)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages\n",
    "\n",
    "We first import all the packages needed for text processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lau\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# if you do not have 'nltk', the following command should work \"python -m pip install nltk\"\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import json\n",
    "import regex as re \n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data into memory\n",
    "\n",
    "The dataset is stored in a txt file ```dataset_tweets_WHO.txt```and it contains a set of tweets in json format. We create tweets_data by using json.loads() function, which from a JSON string it can be parsed and it returns the content of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('dataset_tweets_WHO.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data=[]\n",
    "for line in text:\n",
    "    tweet=json.loads(line)\n",
    "    tweets_data.append(tweet)\n",
    "text.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2399"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_data[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0  \\\n",
      "0  {'created_at': 'Wed Oct 13 09:15:58 +0000 2021...   \n",
      "\n",
      "                                                   1  \\\n",
      "0  {'created_at': 'Wed Oct 13 08:46:17 +0000 2021...   \n",
      "\n",
      "                                                   2  \\\n",
      "0  {'created_at': 'Wed Oct 13 07:53:28 +0000 2021...   \n",
      "\n",
      "                                                   3  \\\n",
      "0  {'created_at': 'Wed Oct 13 05:47:26 +0000 2021...   \n",
      "\n",
      "                                                   4  \\\n",
      "0  {'created_at': 'Wed Oct 13 05:47:10 +0000 2021...   \n",
      "\n",
      "                                                   5  \\\n",
      "0  {'created_at': 'Wed Oct 13 05:44:56 +0000 2021...   \n",
      "\n",
      "                                                   6  \\\n",
      "0  {'created_at': 'Tue Oct 12 22:15:15 +0000 2021...   \n",
      "\n",
      "                                                   7  \\\n",
      "0  {'created_at': 'Tue Oct 12 22:15:12 +0000 2021...   \n",
      "\n",
      "                                                   8  \\\n",
      "0  {'created_at': 'Tue Oct 12 21:01:45 +0000 2021...   \n",
      "\n",
      "                                                   9  ...  \\\n",
      "0  {'created_at': 'Tue Oct 12 21:01:40 +0000 2021...  ...   \n",
      "\n",
      "                                                2389  \\\n",
      "0  {'created_at': 'Fri Jun 25 10:04:00 +0000 2021...   \n",
      "\n",
      "                                                2390  \\\n",
      "0  {'created_at': 'Fri Jun 25 09:05:03 +0000 2021...   \n",
      "\n",
      "                                                2391  \\\n",
      "0  {'created_at': 'Thu Jun 24 21:22:22 +0000 2021...   \n",
      "\n",
      "                                                2392  \\\n",
      "0  {'created_at': 'Thu Jun 24 15:38:09 +0000 2021...   \n",
      "\n",
      "                                                2393  \\\n",
      "0  {'created_at': 'Thu Jun 24 13:51:17 +0000 2021...   \n",
      "\n",
      "                                                2394  \\\n",
      "0  {'created_at': 'Thu Jun 24 12:38:49 +0000 2021...   \n",
      "\n",
      "                                                2395  \\\n",
      "0  {'created_at': 'Thu Jun 24 11:46:20 +0000 2021...   \n",
      "\n",
      "                                                2396  \\\n",
      "0  {'created_at': 'Thu Jun 24 11:41:45 +0000 2021...   \n",
      "\n",
      "                                                2397  \\\n",
      "0  {'created_at': 'Thu Jun 24 11:41:26 +0000 2021...   \n",
      "\n",
      "                                                2398  \n",
      "0  {'created_at': 'Thu Jun 24 11:00:38 +0000 2021...  \n",
      "\n",
      "[1 rows x 2399 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build DataFrame of tweet texts and languages\n",
    "df = pd.DataFrame(tweets_data)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n",
    "\n",
    "#parse dataframe to csv\n",
    "df.to_csv('df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a new variable called 'texts' which will contain in each position of the array a tweet.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[]\n",
    "for i in df:\n",
    "    line =  df[i].item()['full_text']\n",
    "    texts.append(str(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's International Day for Disaster Risk Reduction\\n\\n#OpenWHO has launched a multi-tiered core curriculum to help equip you with the competencies needed to work within public health emergency response.\\n\\nStart learning today &amp; be #Ready4Response:\\nðŸ‘‰ https://t.co/hBFFOF0xKL https://t.co/fgZY22RWuS\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"it's international day for disaster risk reduction\\n\\n#openwho has launched a multi-tiered core curriculum to help equip you with the competencies needed to work within public health emergency response.\\n\\nstart learning today &amp; be #ready4response:\\nðŸ‘‰ https://t.co/hbffof0xkl https://t.co/fgzy22rwus\""
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the function ```build_terms(text)```.\n",
    "\n",
    "It takes as input a text and performs the following operations:\n",
    "\n",
    "- Remove stop words\n",
    "- Stem terms\n",
    "- Transform all text to lowercase\n",
    "- Tokenize the text to get a list of terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_terms(text):\n",
    "    \"\"\"\n",
    "    Preprocess the article text (title + body) removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    text -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    text - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "    # create the pattern\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\"#\", \"â€“\")# don't remove hashtags\n",
    "    remove = remove+'Â¿'\n",
    "    pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "    text=re.sub(pattern, \"\", text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    #compile a regular expression pattern into a regular expression object\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    #Return the string obtained by replacing the leftmost non-overlapping occurrences of pattern in string by the replacement\n",
    "    text = emoji_pattern.sub(r'', text) # no emoji\n",
    "    \n",
    "    # Transform in lowercase\n",
    "    text=  str.lower(text) \n",
    "    # Tokenize the text to get a list of terms\n",
    "    text=  text.split() \n",
    "    # Eliminate the stopwords\n",
    "    text=[l for l in text if l not in stop_words] \n",
    "    \n",
    "\n",
    "    # Perform stemming \n",
    "    text=[stemmer.stem(word) for word in text]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt',\n",
       " 'drtedro',\n",
       " 'donat',\n",
       " 'arent',\n",
       " 'enough',\n",
       " 'deliv',\n",
       " '#vaccinequ',\n",
       " 'end',\n",
       " '#covid19',\n",
       " 'pandem',\n",
       " 'need',\n",
       " 'stronger',\n",
       " 'leadership',\n",
       " 'ramp',\n",
       " 'theâ€¦']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_terms(texts[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#COVID19 has shown how health emergencies and disasters affect entire communities â€“ especially those with weak health systems, and vulnerable populations like migrants, indigenous peoples, and those living in fragile humanitarian conditions. https://t.co/jpUQpnu0V1'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
