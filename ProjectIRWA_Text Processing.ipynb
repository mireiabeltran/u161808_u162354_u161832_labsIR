{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRWA Final Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to build a search engine implementing different indexing and ranking algorithms. This will be done using a file containing a set of tweets from the World Health Organization (@WHO).\n",
    "\n",
    "It will be divided in four parts:\n",
    "\n",
    "    1) Text processing\n",
    "    2) Indexing and ranking\n",
    "    3) Evaluation \n",
    "    4) User Interface and Web analytics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Students Group 9:\n",
    "- Mireia Beltran (U161808)\n",
    "- Cisco Orteu (U162354)\n",
    "- Laura Casanovas (U161832)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages\n",
    "\n",
    "We first import all the packages needed for text processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mire2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# if you do not have 'nltk', the following command should work \"python -m pip install nltk\"\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import json\n",
    "import regex as re \n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data into memory\n",
    "\n",
    "The dataset is stored in a txt file ```dataset_tweets_WHO.txt```and it contains a set of tweets in json format. We create tweets_data by using json.loads() function, which from a JSON string it can be parsed and it returns the content of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('dataset_tweets_WHO.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data=[]\n",
    "for line in text:\n",
    "    tweet=json.loads(line)\n",
    "    tweets_data.append(tweet)\n",
    "text.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to read and process each tweet, we create a dictionary in which we will have just one row, and each column will contain one tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build DataFrame of tweet texts and languages\n",
    "df = pd.DataFrame(tweets_data)\n",
    "df.to_csv('df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a new variable called 'texts' which will contain in each position of the array a tweet. Below this cell we print as an example the content of the first position of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[]\n",
    "for i in df:\n",
    "    line =  df[i].item()['full_text']\n",
    "    texts.append(str(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's International Day for Disaster Risk Reduction\\n\\n#OpenWHO has launched a multi-tiered core curriculum to help equip you with the competencies needed to work within public health emergency response.\\n\\nStart learning today &amp; be #Ready4Response:\\nðŸ‘‰ https://t.co/hBFFOF0xKL https://t.co/fgZY22RWuS\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the function ```build_terms(text)```.\n",
    "\n",
    "It takes as input a text and performs the following operations:\n",
    "\n",
    "- Stem terms\n",
    "- Remove stop words\n",
    "- Remove punctuation \n",
    "- Remove links\n",
    "- Remove emojis\n",
    "- Transform all text to lowercase\n",
    "- Tokenize the text to get a list of terms\n",
    "\n",
    "(We decided not to remove hashtags since it may be interesting to treat them separately later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_terms(text):\n",
    "    \"\"\"\n",
    "    Preprocess the article text (title + body) removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    text -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    text - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "    # create the pattern\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\"#\", \"â€“\")# don't remove hashtags\n",
    "    remove = remove+'Â¿'\n",
    "    pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "    text = re.sub(pattern, \"\", text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    #compile a regular expression pattern into a regular expression object\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    #Return the string obtained by replacing the leftmost non-overlapping occurrences of pattern in string by the replacement\n",
    "    text = emoji_pattern.sub(r'', text) # no emoji\n",
    "    \n",
    "    # Transform in lowercase\n",
    "    text=  str.lower(text) \n",
    "    # Tokenize the text to get a list of terms\n",
    "    text=  text.split() \n",
    "    # Eliminate the stopwords\n",
    "    text=[l for l in text if l not in stop_words] \n",
    "    \n",
    "\n",
    "    # Perform stemming \n",
    "    text=[stemmer.stem(word) for word in text]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we pass the whole list of tweets into the function we just created in order to process them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_processed = []\n",
    "for i in range(len(texts)):\n",
    "    texts_processed.append(build_terms(texts[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we have an example of the first tweet before and after being processed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  #COVID19 has shown how health emergencies and disasters affect entire communities â€“ especially those with weak health systems, and vulnerable populations like migrants, indigenous peoples, and those living in fragile humanitarian conditions. https://t.co/jpUQpnu0V1 \n",
      "\n",
      "After:  ['#covid19', 'shown', 'health', 'emerg', 'disast', 'affect', 'entir', 'commun', 'especi', 'weak', 'health', 'system', 'vulner', 'popul', 'like', 'migrant', 'indigen', 'peopl', 'live', 'fragil', 'humanitarian', 'condit'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Before: ', texts[1],'\\n')\n",
    "print('After: ', texts_processed[1], '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
