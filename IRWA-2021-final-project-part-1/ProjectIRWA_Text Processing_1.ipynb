{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRWA Final Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to build a search engine implementing different indexing and ranking algorithms. This will be done using a file containing a set of tweets from the World Health Organization (@WHO).\n",
    "\n",
    "It will be divided in four parts:\n",
    "\n",
    "    1) Text processing\n",
    "    2) Indexing and ranking\n",
    "    3) Evaluation \n",
    "    4) User Interface and Web analytics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Students Group 9:\n",
    "- Mireia Beltran (U161808)\n",
    "- Cisco Orteu (U162354)\n",
    "- Laura Casanovas (U161832)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages\n",
    "\n",
    "We first import all the packages needed for text processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mire2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# if you do not have 'nltk', the following command should work \"python -m pip install nltk\"\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import json\n",
    "import regex as re \n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data into memory\n",
    "\n",
    "The dataset is stored in a txt file ```dataset_tweets_WHO.txt```and it contains a set of tweets in json format. We create tweets_data by using json.loads() function, which from a JSON string it can be parsed and it returns the content of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('dataset_tweets_WHO.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data=[]\n",
    "for line in text:\n",
    "    tweet=json.loads(line)\n",
    "    tweets_data.append(tweet)\n",
    "text.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to read and process each tweet, we create a dictionary in which we will have just one row, and each column will contain one tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build DataFrame of tweet texts and languages\n",
    "df = pd.DataFrame(tweets_data)\n",
    "df.to_csv('df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a new variable called 'texts' which will contain in each position of the array a tweet. Below this cell we print as an example the content of the first position of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[]\n",
    "for i in df:\n",
    "    line =  df[i].item()['full_text']\n",
    "    texts.append(str(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's International Day for Disaster Risk Reduction\\n\\n#OpenWHO has launched a multi-tiered core curriculum to help equip you with the competencies needed to work within public health emergency response.\\n\\nStart learning today &amp; be #Ready4Response:\\nðŸ‘‰ https://t.co/hBFFOF0xKL https://t.co/fgZY22RWuS\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the function ```build_terms(text)```.\n",
    "\n",
    "It takes as input a text and performs the following operations:\n",
    "\n",
    "- Stem terms\n",
    "- Remove stop words\n",
    "- Remove punctuation \n",
    "- Remove links\n",
    "- Remove emojis\n",
    "- Transform all text to lowercase\n",
    "- Tokenize the text to get a list of terms\n",
    "\n",
    "(We decided not to remove hashtags since it may be interesting to treat them separately later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_terms(text):\n",
    "    \"\"\"\n",
    "    Preprocess the article text (title + body) removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    text -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    text - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "    # create the pattern\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\"#\", \"â€“\")# don't remove hashtags\n",
    "    remove = remove+'Â¿'\n",
    "    pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "    text = re.sub(pattern, \"\", text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    #compile a regular expression pattern into a regular expression object\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    #Return the string obtained by replacing the leftmost non-overlapping occurrences of pattern in string by the replacement\n",
    "    text = emoji_pattern.sub(r'', text) # no emoji\n",
    "    \n",
    "    # Transform in lowercase\n",
    "    text=  str.lower(text) \n",
    "    # Tokenize the text to get a list of terms\n",
    "    text=  text.split() \n",
    "    # Eliminate the stopwords\n",
    "    text=[l for l in text if l not in stop_words] \n",
    "    \n",
    "\n",
    "    # Perform stemming \n",
    "    text=[stemmer.stem(word) for word in text]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_processed = []\n",
    "for i in range(len(texts)):\n",
    "    texts_processed.append(build_terms(texts[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(texts):\n",
    "    \"\"\"\n",
    "    Implement the inverted index\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of documents where these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    index = defaultdict(list)\n",
    "    title_index = {}  # dictionary to map page titles to page ids\n",
    "    tweet_id = 0\n",
    "    for text in texts:  # Remember, lines contain all documents\n",
    "        terms = build_terms(text) #page_title + page_text\n",
    "        tweet_id += 1\n",
    "        title_index[tweet_id]=text  \n",
    "        \n",
    "        ## ===============================================================        \n",
    "        ## create the index for the current page and store it in current_page_index (current_page_index)\n",
    "        ## current_page_index ==> { â€˜term1â€™: [current_doc, [list of positions]], ...,â€˜term_nâ€™: [current_doc, [list of positions]]}\n",
    "\n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ##\"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_page_index ==> { â€˜webâ€™: [1, [0]], â€˜retrievalâ€™: [1, [1,4]], â€˜informationâ€™: [1, [2]]}\n",
    "\n",
    "        ## the term â€˜webâ€™ appears in document 1 in positions 0, \n",
    "        ## the term â€˜retrievalâ€™ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_page_index = {}\n",
    "\n",
    "        for term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (current_page_index)\n",
    "                # append the position to the corresponding list\n",
    "                \n",
    "        ## START CODE\n",
    "                current_page_index[term][1].append(tweet_id)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_page_index[term]=tweet_id #'I' indicates unsigned int (int in Python)\n",
    "            \n",
    "        #merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "        \n",
    "        ## END CODE                    \n",
    "                    \n",
    "    return index, title_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "index, title_index= create_index(texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
